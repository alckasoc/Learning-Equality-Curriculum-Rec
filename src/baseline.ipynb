{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b12c4c-2e4a-486b-8a34-188e87e3eef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nvidia_smi\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89280ccd-a817-4157-ad31-29aa4726d11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom imports.\n",
    "from utils import f2_score, seed_everything, AverageMeter, timeSince, get_vram, get_param_counts\n",
    "from baseline_utils.dataset import custom_dataset, collate\n",
    "from baseline_utils.model import custom_model\n",
    "from baseline_utils.utils import get_max_length, get_best_threshold, get_optimizer_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8074e9-0183-4c68-b4cf-b75b5ca66cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincenttu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba7fc69-fbd1-4d38-a8e7-966048200764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.epochs = epochs\n",
    "        self.model = \"xlm-roberta-base\"\n",
    "        self.correlations = \"../input/correlations.csv\"\n",
    "        self.train = \"../input/train_5fold.csv\"\n",
    "        self.project = \"LECR_0.297_baseline\"\n",
    "        self.project_run_root = \"test\"\n",
    "        self.save_root = \"../models/test/\"\n",
    "        self.fold = 0\n",
    "        self.patience = 1\n",
    "        self.debug = 0\n",
    "        self.gradient_checkpointing = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18e3602b-9aaf-4014-9ec7-7a9e1c65f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "\n",
    "epochs = args.epochs\n",
    "correlations = args.correlations\n",
    "train = args.train\n",
    "project = args.project\n",
    "project_run_root = args.project_run_root\n",
    "save_root = args.save_root\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "fold = args.fold\n",
    "patience = args.patience\n",
    "gradient_checkpointing = args.gradient_checkpointing\n",
    "\n",
    "seed = 42\n",
    "\n",
    "class CFG:\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "    model = args.model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 32\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    max_len = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68fbb1b1-5b55-49a3-b71d-cafb3f65ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================================\n",
    "# Train function loop\n",
    "# =========================================================================================\n",
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = True):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, \n",
    "                          step, \n",
    "                          len(train_loader), \n",
    "                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "            if args.debug:\n",
    "                get_vram()\n",
    "        if not args.debug and step % (cfg.print_freq * 6) == 0:\n",
    "            get_vram()\n",
    "\n",
    "    return losses.avg\n",
    "\n",
    "# =========================================================================================\n",
    "# Valid function loop\n",
    "# =========================================================================================\n",
    "def valid_fn(valid_loader, model, criterion, device, cfg):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, \n",
    "                          len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "            \n",
    "        if step % (cfg.print_freq * 6) == 0:\n",
    "            get_vram()\n",
    "            \n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    \n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf5a3b87-20d1-4647-81d9-e95be33e8727",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 615170/615170 [00:44<00:00, 13848.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Seed everything\n",
    "seed_everything(seed)\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# Read data\n",
    "correlations = pd.read_csv(\"../input/correlations.csv\")\n",
    "train = pd.read_csv(\"../input/train_5fold.csv\")\n",
    "\n",
    "# Get max length\n",
    "get_max_length(train, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d91c950d-0b5d-4235-9586-88a25464fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train & validation.\n",
    "x_train = train[train['fold'] != fold]\n",
    "x_val = train[train['fold'] == fold]\n",
    "valid_labels = x_val['target'].values\n",
    "train_dataset = custom_dataset(x_train, cfg)\n",
    "valid_dataset = custom_dataset(x_val, cfg)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size = cfg.batch_size, \n",
    "    shuffle = True, \n",
    "    num_workers = cfg.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = True\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size = cfg.batch_size, \n",
    "    shuffle = False, \n",
    "    num_workers = cfg.num_workers, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a07c721-ac78-47c8-8017-1c7b7973491d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Model.\n",
    "model = custom_model(cfg, gradient_checkpointing=gradient_checkpointing)\n",
    "_ = model.to(device)\n",
    "\n",
    "# Optimizer.\n",
    "optimizer_parameters = get_optimizer_params(\n",
    "    model, \n",
    "    encoder_lr = cfg.encoder_lr, \n",
    "    decoder_lr = cfg.decoder_lr,\n",
    "    weight_decay = cfg.weight_decay\n",
    ")\n",
    "optimizer = AdamW(\n",
    "    optimizer_parameters, \n",
    "    lr = cfg.encoder_lr, \n",
    "    eps = cfg.eps, \n",
    "    betas = cfg.betas\n",
    ")\n",
    "\n",
    "# Scheduler.\n",
    "num_train_steps = int(len(x_train) / cfg.batch_size * epochs)\n",
    "num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps = num_warmup_steps, \n",
    "    num_training_steps = num_train_steps, \n",
    "    num_cycles = cfg.num_cycles\n",
    "    )\n",
    "\n",
    "# Criterion.\n",
    "criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f4893e5-ba91-47a9-9b2c-54e960e082cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing GPU stats...\n",
      "Device 0: b'NVIDIA A40', Memory : (74.83% free): 13.496484911069274 (total), 10.100000536069274 (free), 3.396484375 (used)\n"
     ]
    }
   ],
   "source": [
    "print(\"Printing GPU stats...\")\n",
    "get_vram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9dec669-3f7c-41b6-8edc-36c3a7e2588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing GPU stats...\n",
      "Device 0: b'NVIDIA A40', Memory : (74.83% free): 13.496484911069274 (total), 10.100000536069274 (free), 3.396484375 (used)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.7 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/vincenttu/LECR_0.297_baseline/runs/31qhm5h5\" target=\"_blank\">test_fold0</a></strong> to <a href=\"https://wandb.ai/vincenttu/LECR_0.297_baseline\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Project configuration.\n",
    "print(\"Printing GPU stats...\")\n",
    "get_vram()\n",
    "\n",
    "cfg_params = [i for i in dir(cfg) if \"__\" not in i]\n",
    "cfg_params = dict(zip(cfg_params, [getattr(cfg, i) for i in cfg_params]))\n",
    "total_params, trainable_params, nontrainable_params = get_param_counts(model)\n",
    "cfg_params.update({\n",
    "    \"total_params\": total_params,\n",
    "    \"trainable_params\": trainable_params,\n",
    "    \"nontrainable_params\": nontrainable_params\n",
    "})\n",
    "\n",
    "# Initialize run.\n",
    "run = wandb.init(project=project, config=cfg_params, name=f\"{project_run_root}_fold{fold}\", dir=\"/tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2112c590-f1bf-4439-bb73-6ebeb15f3904",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/15381] Elapsed 0m 0s (remain 74m 9s) Loss: 0.8532(0.8532) Grad: inf  LR: 0.00000000  \n",
      "Device 0: b'NVIDIA A40', Memory : (61.40% free): 13.496484911069274 (total), 8.287500536069274 (free), 5.208984375 (used)\n",
      "Epoch: [1][500/15381] Elapsed 0m 48s (remain 24m 4s) Loss: 0.5878(0.7907) Grad: 6.3956  LR: 0.00000065  \n",
      "Device 0: b'NVIDIA A40', Memory : (37.69% free): 13.496484911069274 (total), 5.086328661069274 (free), 8.41015625 (used)\n",
      "Epoch: [1][1000/15381] Elapsed 1m 37s (remain 23m 25s) Loss: 0.2023(0.5462) Grad: 4.6545  LR: 0.00000130  \n",
      "Device 0: b'NVIDIA A40', Memory : (37.22% free): 13.496484911069274 (total), 5.023828661069274 (free), 8.47265625 (used)\n",
      "Epoch: [1][1500/15381] Elapsed 2m 26s (remain 22m 38s) Loss: 0.1050(0.4402) Grad: 9.4569  LR: 0.00000195  \n",
      "Device 0: b'NVIDIA A40', Memory : (37.08% free): 13.496484911069274 (total), 5.004297411069274 (free), 8.4921875 (used)\n",
      "Epoch: [1][2000/15381] Elapsed 3m 15s (remain 21m 47s) Loss: 0.2976(0.3828) Grad: 7.6789  LR: 0.00000260  \n",
      "Device 0: b'NVIDIA A40', Memory : (34.82% free): 13.496484911069274 (total), 4.699609911069274 (free), 8.796875 (used)\n",
      "Epoch: [1][2500/15381] Elapsed 4m 4s (remain 21m 0s) Loss: 0.2006(0.3475) Grad: 8.0548  LR: 0.00000325  \n",
      "Device 0: b'NVIDIA A40', Memory : (31.61% free): 13.496484911069274 (total), 4.266016161069274 (free), 9.23046875 (used)\n",
      "Epoch: [1][3000/15381] Elapsed 4m 53s (remain 20m 11s) Loss: 0.1220(0.3217) Grad: 6.9719  LR: 0.00000390  \n",
      "Device 0: b'NVIDIA A40', Memory : (31.61% free): 13.496484911069274 (total), 4.266016161069274 (free), 9.23046875 (used)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭──────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1882/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">978766885.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1882/978766885.py'</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_1882/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">106053499.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train_fn</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_1882/106053499.py'</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/envs/python37/lib/python3.7/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">338</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">335 │   │   </span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">336 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>]) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"No inf checks wer</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337 │   │   </span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>338 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retval = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">339 │   │   </span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">340 │   │   </span>optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"stage\"</span>] = OptState.STEPPED                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">341 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/envs/python37/lib/python3.7/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">284</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, optimizer_state, *args, **kwargs):         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 │   │   </span>retval = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>284 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sum</span>(v.item() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>].values <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">285 │   │   │   </span>retval = optimizer.step(*args, **kwargs)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> retval                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/envs/python37/lib/python3.7/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">284</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;genexpr&gt;</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">281 │   </span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, optimizer_state, *args, **kwargs):         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 │   │   </span>retval = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>284 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sum</span>(v.item() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>].values <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">285 │   │   │   </span>retval = optimizer.step(*args, **kwargs)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> retval                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰───────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m─────────────────────────── \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m ───────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1882/\u001b[0m\u001b[1;33m978766885.py\u001b[0m:\u001b[94m7\u001b[0m in \u001b[92m<module>\u001b[0m                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1882/978766885.py'\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_1882/\u001b[0m\u001b[1;33m106053499.py\u001b[0m:\u001b[94m23\u001b[0m in \u001b[92mtrain_fn\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_1882/106053499.py'\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/envs/python37/lib/python3.7/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m338\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mstep\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m335 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m]) > \u001b[94m0\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mNo inf checks wer\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m337 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m338 \u001b[2m│   │   \u001b[0mretval = \u001b[96mself\u001b[0m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m339 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer_state[\u001b[33m\"\u001b[0m\u001b[33mstage\u001b[0m\u001b[33m\"\u001b[0m] = OptState.STEPPED                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m341 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/envs/python37/lib/python3.7/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m284\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_maybe_opt_step\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_maybe_opt_step\u001b[0m(\u001b[96mself\u001b[0m, optimizer, optimizer_state, *args, **kwargs):         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2m│   │   \u001b[0mretval = \u001b[94mNone\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m284 \u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96msum\u001b[0m(v.item() \u001b[94mfor\u001b[0m v \u001b[95min\u001b[0m optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m].values \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m285 \u001b[0m\u001b[2m│   │   │   \u001b[0mretval = optimizer.step(*args, **kwargs)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m retval                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/envs/python37/lib/python3.7/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m284\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m<genexpr>\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m281 \u001b[0m\u001b[2m│   \u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_maybe_opt_step\u001b[0m(\u001b[96mself\u001b[0m, optimizer, optimizer_state, *args, **kwargs):         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2m│   │   \u001b[0mretval = \u001b[94mNone\u001b[0m                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m284 \u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96msum\u001b[0m(v.item() \u001b[94mfor\u001b[0m v \u001b[95min\u001b[0m optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m].values \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m285 \u001b[0m\u001b[2m│   │   │   \u001b[0mretval = optimizer.step(*args, **kwargs)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m retval                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰───────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training & validation loop.\n",
    "best_score, cnt = 0, 0\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train.\n",
    "    avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n",
    "\n",
    "    # Validation.\n",
    "    avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n",
    "\n",
    "    # Compute f2_score.\n",
    "    score, threshold = get_best_threshold(x_val, predictions, correlations)\n",
    "\n",
    "    # Logging.\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "    print(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n",
    "\n",
    "    run.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"avg_train_loss\": avg_loss,\n",
    "        \"avg_val_loss\": avg_val_loss,\n",
    "        \"f2_score\": score,\n",
    "        \"threshold\": threshold\n",
    "    })\n",
    "\n",
    "    # Saving & early stopping.\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "        save_m = f\"{cfg.model.replace('/', '-')}_fold{fold}_ep{epoch}.pth\"\n",
    "        save_p = os.path.join(save_root, save_m)\n",
    "        torch.save(model.state_dict(), save_p)\n",
    "\n",
    "        # W&B save model as artifact.\n",
    "        artifact = wandb.Artifact(cfg.model.replace('/', '-'), type='model')\n",
    "        artifact.add_file(save_p, name=f\"fold{fold}_ep{epoch}.pth\")\n",
    "        run.log_artifact(artifact)\n",
    "\n",
    "        val_predictions = predictions\n",
    "    elif patience != -1 and patience > 0:\n",
    "        cnt += 1\n",
    "        if cnt == patience:\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            save_p = os.path.join(save_root, f\"{cfg.model.replace('/', '-')}_fold{fold}_ep{epoch}.pth\")\n",
    "            torch.save(model.state_dict(), save_p)\n",
    "\n",
    "            # W&B save model as artifact.\n",
    "            artifact = wandb.Artifact(cfg.model.replace('/', '-'), type='model')\n",
    "            artifact.add_file(save_p, name=f\"fold{fold}_ep{epoch}.pth\")\n",
    "            run.log_artifact(artifact)\n",
    "\n",
    "            val_predictions = predictions\n",
    "            break\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Get best threshold.\n",
    "best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n",
    "print(f'Our CV score is {best_score} using a threshold of {best_threshold}')\n",
    "\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c9283b-0ad9-4366-a8f4-bf7b95b1afd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 2097... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">test_fold0</strong>: <a href=\"https://wandb.ai/vincenttu/LECR_0.297_baseline/runs/31qhm5h5\" target=\"_blank\">https://wandb.ai/vincenttu/LECR_0.297_baseline/runs/31qhm5h5</a><br/>\n",
       "Find logs at: <code>/tmp/wandb/run-20230109_074819-31qhm5h5/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238449fb-75b1-400c-8af9-67bca161b93e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

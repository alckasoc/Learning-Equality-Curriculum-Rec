utils:
    debug: 0

    correlations: "../../input/correlations.csv"
    train: "../../input/prep_cleaned_train_context_5fold.csv"
    project: "FPELL_rank1_LECR"
    project_run_root: "test0"
    save_root: "../../models/test0"
    
    num_folds: 5

    num_workers: 0

training:
    epochs: 3

    train_batch_size: 24
    val_batch_size: 24

    max_length: 1024

    gradient_accumulation_steps: 1
    max_grad_norm: 1000
    unscale: False
    patience: 1
    
    evaluate_n_times_per_epoch: 2

model:
    tokenizer_path: '../../input/model23/tokenizer/'

    backbone_type: 'microsoft/deberta-v3-large'
    pretrained_backbone: True
    from_checkpoint: True
    model_checkpoint_path: '../../input/model23/microsoft-deberta-v3-large_fold0_best.pth'

    backbone_cfg:
        backbone_config_path: '../../input/model23/config.pth'

        backbone_hidden_dropout: 0.
        backbone_hidden_dropout_prob: 0.
        backbone_attention_dropout: 0.
        backbone_attention_probs_dropout_prob: 0.

    pooling_type: 'MeanPooling' # ['MeanPooling', 'ConcatPooling', 'WeightedLayerPooling', 'GRUPooling', 'LSTMPooling', 'AttentionPooling']
    pooling_cfg:
        gru_pooling:
            hidden_size: 1024
            dropout_rate: 0.1
            bidirectional: False

        weighted_pooling:
            layer_start: 8
            layer_weights: null

        wk_pooling:
            layer_start: 4
            context_window_size: 2

        lstm_pooling:
            hidden_size: 1024
            dropout_rate: 0.1
            bidirectional: False
            
        attention_pooling:
            hiddendim_fc: 1024
            dropout: 0.1
            
        concat_pooling:
            n_layers: 4

    gradient_checkpointing: True

    freeze_embeddings: False
    freeze_n_layers: 0
    reinitialize_n_layers: 0

optimizer:
    use_swa: False
    
    swa_cfg:
        swa_start: 10
        swa_freq: 10
        swa_lr: 0.0005

    encoder_lr: 0.000002
    embeddings_lr: 0.0000015
    decoder_lr: 0.000009

    group_lt_multiplier: 0.95
    n_groups: 6

    eps: 1.e-6
    betas: [0.9, 0.999]

    weight_decay: 0.01

scheduler:
    scheduler_type: 'cosine_schedule_with_warmup' # [constant_schedule_with_warmup, linear_schedule_with_warmup, cosine_schedule_with_warmup,polynomial_decay_schedule_with_warmup]
    batch_scheduler: True

    scheduler_cfg:
        constant_schedule_with_warmup:
            n_warmup_steps: 0

        linear_schedule_with_warmup:
            n_warmup_steps: 0

        cosine_schedule_with_warmup:
            n_cycles: 0.5
            n_warmup_steps: 0

        polynomial_decay_schedule_with_warmup:
            n_warmup_steps: 0
            power: 1.0
            min_lr: 0.0

adversarial_learning:
    adversarial_lr: 0.00001
    adversarial_eps: 0.001
    adversarial_epoch_start: 2

n_workers: 4

train_print_frequency: 100
valid_print_frequency: 50

train_batch_size: 3
valid_batch_size: 3

seed: 42

n_folds: 5

max_length: 1024

model:
    backbone_type: 'microsoft/deberta-v3-large'
    pretrained_backbone: True
    from_checkpoint: True
    checkpoint_id: 'model23_pretrain'

    backbone_config_path: ''
    backbone_hidden_dropout: 0.
    backbone_hidden_dropout_prob: 0.
    backbone_attention_dropout: 0.
    backbone_attention_probs_dropout_prob: 0.

    pooling_type: 'MeanPooling' # ['MeanPooling', 'ConcatPooling', 'WeightedLayerPooling', 'GRUPooling', 'LSTMPooling', 'AttentionPooling']

    gru_pooling:
        hidden_size: 1024
        dropout_rate: 0.1
        bidirectional: False

    weighted_pooling:
        layer_start: 8
        layer_weights: null

    wk_pooling:
        layer_start: 4
        context_window_size: 2

    lstm_pooling:
        hidden_size: 1024
        dropout_rate: 0.1
        bidirectional: False
        
    attention_pooling:
        hiddendim_fc: 1024
        dropout: 0.1
        
    concat_pooling:
        n_layers: 4

    gradient_checkpointing: True

    freeze_embeddings: False
    freeze_n_layers: 0
    reinitialize_n_layers: 0


optimizer:
    use_swa: False
    
    swa:
        swa_start: 10
        swa_freq: 10
        swa_lr: 0.0005

    encoder_lr: 0.000002
    embeddings_lr: 0.0000015
    decoder_lr: 0.000009

    group_lt_multiplier: 0.95
    n_groups: 6

    eps: 1.e-6
    betas: [0.9, 0.999]

    weight_decay: 0.01

scheduler:
    scheduler_type: 'cosine_schedule_with_warmup' # [constant_schedule_with_warmup, linear_schedule_with_warmup, cosine_schedule_with_warmup,polynomial_decay_schedule_with_warmup]
    batch_scheduler: True

    constant_schedule_with_warmup:
        n_warmup_steps: 0

    linear_schedule_with_warmup:
        n_warmup_steps: 0

    cosine_schedule_with_warmup:
        n_cycles: 0.5
        n_warmup_steps: 0

    polynomial_decay_schedule_with_warmup:
        n_warmup_steps: 0
        power: 1.0
        min_lr: 0.0

adversarial_learning:
    adversarial_lr: 0.00001
    adversarial_eps: 0.001
    adversarial_epoch_start: 2

training:
    epochs: 3
    apex: True
    gradient_accumulation_steps: 1
    evaluate_n_times_per_epoch: 2
    max_grad_norm: 1000
    unscale: False

parser.add_argument("--epochs", default=5, type=int)
parser.add_argument("--model", default="xlm-roberta-base", type=str)
parser.add_argument("--correlations", default="../input/correlations.csv", type=str)
parser.add_argument("--train", default="../input/train_5fold.csv", type=str)
parser.add_argument("--project", default="LECR_0.297_baseline", type=str)
parser.add_argument("--project_run_root", default="test", type=str)
parser.add_argument("--save_root", default="../models/0297_baseline/", type=str)
parser.add_argument("--fold", default=0, type=int)
parser.add_argument("--patience", default=1, type=int)
parser.add_argument("--debug", default=0, type=int)
parser.add_argument("--gradient_checkpointing", default=0, type=int)
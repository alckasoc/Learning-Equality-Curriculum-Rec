{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5efcd0-2680-4af7-8c7c-309844ea30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55278d7e-7bf5-4b54-9c5b-b5762314fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from pooling import *\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, model, pooling_type, hidden_size=None, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.pooling_type = pooling_type\n",
    "        \n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "            \n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(model, config=self.config)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(self.config)\n",
    "        \n",
    "        if pooling_type == 'MeanPooling':\n",
    "            self.pool = MeanPooling()\n",
    "        elif pooling_type == 'WeightedLayerPooling':\n",
    "            self.pool = WeightedLayerPooling(self.config.num_hidden_layers)\n",
    "        elif pooling_type == 'LSTMPooling':\n",
    "            self.pool =  LSTMPooling(self.config.num_hidden_layers,\n",
    "                                       self.config.hidden_size,\n",
    "                                       hidden_size,\n",
    "                                       0.1,\n",
    "                                       is_lstm=True\n",
    "                           )\n",
    "        else:\n",
    "            raise ValueError('Unknown pooling type')\n",
    "        \n",
    "        \n",
    "        if pooling_type == 'GRUPooling':\n",
    "            self.fc = nn.Linear(hidden_size, 6)\n",
    "        elif pooling_type == 'LSTMPooling':\n",
    "            self.fc = nn.Linear(hidden_size, 6)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.config.hidden_size, 6)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.backbone(**inputs)\n",
    "        \n",
    "        last_hidden_states = outputs[0]\n",
    "        \n",
    "        if self.pooling_type == 'MeanPooling':\n",
    "            feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        elif self.pooling_type == 'WeightedLayerPooling':\n",
    "            all_hidden_states = torch.stack(outputs[1])\n",
    "            feature = self.pool(all_hidden_states)\n",
    "        elif self.pooling_type in ['GRUPooling', 'LSTMPooling']:\n",
    "            all_hidden_states = torch.stack(outputs[1])\n",
    "            feature = self.pool(all_hidden_states)\n",
    "        else:\n",
    "            raise ValueError('Unknown pooling type')\n",
    "        \n",
    "        return outputs, feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        original_outputs, feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return original_outputs, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f785e21a-29d9-4be6-8294-3f28209df791",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = CustomModel(\"microsoft/deberta-v3-large\", \"MeanPooling\", hidden_size=None, config_path=\"../../../input/model23/config.pth\", pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2cf209-98dd-4546-a5ba-3707474c08fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Config {\n",
       "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-07,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_relative_positions\": -1,\n",
       "  \"model_type\": \"deberta-v2\",\n",
       "  \"norm_rel_ebd\": \"layer_norm\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_dropout\": 0,\n",
       "  \"pooler_hidden_act\": \"gelu\",\n",
       "  \"pooler_hidden_size\": 1024,\n",
       "  \"pos_att_type\": [\n",
       "    \"p2c\",\n",
       "    \"c2p\"\n",
       "  ],\n",
       "  \"position_biased_input\": false,\n",
       "  \"position_buckets\": 256,\n",
       "  \"relative_attention\": true,\n",
       "  \"share_att_key\": true,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 0,\n",
       "  \"vocab_size\": 128015\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16dab901-aa1a-472b-a724-7f2c633672e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.fc = nn.Linear(in_features=1024, out_features=2, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "646012c6-cd1e-40a4-b2e1-992cc65154cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "083b1e01-29bf-493a-be38-779ddf1e0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ({'input_ids': torch.Tensor([[    0, 48962, 14602,   959, 32316,  1065,   294, 21290,   268, 16734,\n",
    "         17991, 14602,   959, 32316,     2,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1]]).to(torch.int32), 'attention_mask': torch.Tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0]]).to(torch.int32)},\n",
    " torch.Tensor([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc7b1516-5848-4ad4-bfdb-0eaa1339819e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[    0, 48962, 14602,   959, 32316,  1065,   294, 21290,   268, 16734,\n",
       "           17991, 14602,   959, 32316,     2,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "               1,     1]], dtype=torch.int32),\n",
       "  'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0]], dtype=torch.int32)},\n",
       " tensor([0.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6a95723-4e9f-490d-92de-6a73f30d1c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m1.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    o, z = m1(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4d00b2-e838-4900-b0e7-df1eadee564e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2601, -0.3926]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2bbeac7-0da2-4f90-b438-f4dbb8551673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['attentions',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'hidden_states',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'last_hidden_state',\n",
       " 'move_to_end',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'to_tuple',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(o) if \"__\" not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33afd827-e216-4a2e-87d1-418cf6e2c8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0628, -0.6051, -0.4633,  ..., -1.6161,  0.3332, -0.7899],\n",
       "         [-0.1748, -0.9188, -0.5154,  ..., -1.0968,  0.7967, -0.9400],\n",
       "         [-0.0848, -1.0519, -0.4905,  ..., -1.3074,  0.2581, -0.5332],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdabbb6d-7884-4625-962f-f22098ce3bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = m1.pool(o[0], x[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e69d14-6729-4e2a-ac26-0f4d97f071c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4ccf766-3522-4a1b-9377-415092dc1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoConfig\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from utils import get_backbone_config\n",
    "\n",
    "def get_last_hidden_state(backbone_outputs):\n",
    "    last_hidden_state = backbone_outputs[0]\n",
    "    return last_hidden_state\n",
    "\n",
    "\n",
    "def get_all_hidden_states(backbone_outputs):\n",
    "    all_hidden_states = torch.stack(backbone_outputs[1])\n",
    "    return all_hidden_states\n",
    "\n",
    "\n",
    "def get_input_ids(inputs):\n",
    "    return inputs['input_ids']\n",
    "\n",
    "\n",
    "def get_attention_mask(inputs):\n",
    "    return inputs['attention_mask']\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        self.output_dim = 1024\n",
    "\n",
    "    def forward(self, inputs, backbone_outputs):  # x, o\n",
    "        attention_mask = get_attention_mask(inputs)\n",
    "        last_hidden_state = get_last_hidden_state(backbone_outputs)\n",
    "\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "class custom_model(nn.Module):\n",
    "    def __init__(self, tokenizer, backbone_type):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        if True:\n",
    "            self.backbone_config = get_backbone_config(backbone_type)\n",
    "            self.backbone = AutoModel.from_pretrained(backbone_type, config=self.backbone_config)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(self.backbone_config)\n",
    "\n",
    "        # What is this?\n",
    "        self.backbone.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        a = len(self.tokenizer) == self.backbone_config.vocab_size\n",
    "        print(f\"len tokenizer vs backbone config vocab size: {a}\")\n",
    "        \n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.pool.output_dim, 6)\n",
    "\n",
    "#         self._init_weights(self.fc)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.backbone(**inputs)\n",
    "        \n",
    "        feature = self.pool(inputs, outputs)\n",
    "        output = self.fc(feature)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c523ea-d954-403f-bdb5-2c5e66147490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def get_additional_special_tokens():\n",
    "    special_tokens_replacement = {\n",
    "        '\\n': '[BR]',\n",
    "        'Generic_School': '[GENERIC_SCHOOL]',\n",
    "        'Generic_school': '[GENERIC_SCHOOL]',\n",
    "        'SCHOOL_NAME': '[SCHOOL_NAME]',\n",
    "        'STUDENT_NAME': '[STUDENT_NAME]',\n",
    "        'Generic_Name': '[GENERIC_NAME]',\n",
    "        'Genric_Name': '[GENERIC_NAME]',\n",
    "        'Generic_City': '[GENERIC_CITY]',\n",
    "        'LOCATION_NAME': '[LOCATION_NAME]',\n",
    "        'HOTEL_NAME': '[HOTEL_NAME]',\n",
    "        'LANGUAGE_NAME': '[LANGUAGE_NAME]',\n",
    "        'PROPER_NAME': '[PROPER_NAME]',\n",
    "        'OTHER_NAME': '[OTHER_NAME]',\n",
    "        'PROEPR_NAME': '[PROPER_NAME]',\n",
    "        'RESTAURANT_NAME': '[RESTAURANT_NAME]',\n",
    "        'STORE_NAME': '[STORE_NAME]',\n",
    "        'TEACHER_NAME': '[TEACHER_NAME]',\n",
    "    }\n",
    "    return special_tokens_replacement\n",
    "\n",
    "special_tokens_replacement = get_additional_special_tokens()\n",
    "all_special_tokens = list(special_tokens_replacement.values())\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\",\n",
    "                                      use_fast=True,\n",
    "                                      additional_special_tokens=all_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06e4a76b-b896-48c6-aa9d-c8b6a9cbff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens_replacement = {\n",
    "    '\\n': '[BR]',\n",
    "    'Generic_School': '[GENERIC_SCHOOL]',\n",
    "    'Generic_school': '[GENERIC_SCHOOL]',\n",
    "    'SCHOOL_NAME': '[SCHOOL_NAME]',\n",
    "    'STUDENT_NAME': '[STUDENT_NAME]',\n",
    "    'Generic_Name': '[GENERIC_NAME]',\n",
    "    'Genric_Name': '[GENERIC_NAME]',\n",
    "    'Generic_City': '[GENERIC_CITY]',\n",
    "    'LOCATION_NAME': '[LOCATION_NAME]',\n",
    "    'HOTEL_NAME': '[HOTEL_NAME]',\n",
    "    'LANGUAGE_NAME': '[LANGUAGE_NAME]',\n",
    "    'PROPER_NAME': '[PROPER_NAME]',\n",
    "    'OTHER_NAME': '[OTHER_NAME]',\n",
    "    'PROEPR_NAME': '[PROPER_NAME]',\n",
    "    'RESTAURANT_NAME': '[RESTAURANT_NAME]',\n",
    "    'STORE_NAME': '[STORE_NAME]',\n",
    "    'TEACHER_NAME': '[TEACHER_NAME]',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a729a52d-c966-4c33-b4de-02f6b919019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len((list(special_tokens_replacement.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c69ac787-dcee-4ae4-b3e6-feac2ff61f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]', 'additional_special_tokens': ['[BR]', '[GENERIC_SCHOOL]', '[GENERIC_SCHOOL]', '[SCHOOL_NAME]', '[STUDENT_NAME]', '[GENERIC_NAME]', '[GENERIC_NAME]', '[GENERIC_CITY]', '[LOCATION_NAME]', '[HOTEL_NAME]', '[LANGUAGE_NAME]', '[PROPER_NAME]', '[OTHER_NAME]', '[PROPER_NAME]', '[RESTAURANT_NAME]', '[STORE_NAME]', '[TEACHER_NAME]']})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d74352c-f487-403e-ab33-b7b9f2d037e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len tokenizer vs backbone config vocab size: True\n"
     ]
    }
   ],
   "source": [
    "m2 = custom_model(tokenizer, \"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f2bcc53-f57f-482d-9d70-68c8224e3468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128015"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.backbone_config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f843970-5cf6-41b7-b1d3-0f496c10aa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\alcka\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "t = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\",\n",
    "                                      use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "928b64fd-6009-43ae-85d4-32632f227b22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec2ea69e-37ef-4598-ac76-b191f3b69cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128015"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d624dbe3-a6ae-4824-b064-2728edb128ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adeb3344-a9e6-4a8e-8328-a59644c468bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2.load_state_dict(m1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dbc9729-405c-4a7a-a0d2-2ab1bbf6a7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = m2.eval()\n",
    "with torch.no_grad():\n",
    "    z = m2(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeda0770-a692-4c35-a248-5a4577efc1ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0079,  0.0859, -0.3627, -1.0221,  0.6245,  0.0832]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56532c73-d28f-457e-b845-26c2d7b18aaf",
   "metadata": {},
   "source": [
    "tensor([[ 0.0079,  0.0859, -0.3627, -1.0221,  0.6245,  0.0832]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b5221b3-2aa8-4c49-bcd9-71da2cc42f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0575, -1.1382, -0.3632,  ..., -1.5897,  0.6908, -0.3918]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19df2ae-2885-49aa-b0e7-9080b1b84310",
   "metadata": {},
   "source": [
    "m1.pool(o, x[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41cab033-bacb-4506-ad2d-5032004496d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ = m2.pool(x[0], o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ecaefefe-fcc2-4734-aa00-43ce432cd78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1024)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(feature_ == feature).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a412369-b967-4af9-8141-46946c9af121",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-large\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "512aa280-be2c-4b63-8245-80390dfbbd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Config {\n",
       "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-07,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_relative_positions\": -1,\n",
       "  \"model_type\": \"deberta-v2\",\n",
       "  \"norm_rel_ebd\": \"layer_norm\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_dropout\": 0,\n",
       "  \"pooler_hidden_act\": \"gelu\",\n",
       "  \"pooler_hidden_size\": 1024,\n",
       "  \"pos_att_type\": [\n",
       "    \"p2c\",\n",
       "    \"c2p\"\n",
       "  ],\n",
       "  \"position_biased_input\": false,\n",
       "  \"position_buckets\": 256,\n",
       "  \"relative_attention\": true,\n",
       "  \"share_att_key\": true,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 0,\n",
       "  \"vocab_size\": 128100\n",
       "}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f7a5996e-23d4-4d0a-ab28-75187db21b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Config {\n",
       "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-07,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_relative_positions\": -1,\n",
       "  \"model_type\": \"deberta-v2\",\n",
       "  \"norm_rel_ebd\": \"layer_norm\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_dropout\": 0,\n",
       "  \"pooler_hidden_act\": \"gelu\",\n",
       "  \"pooler_hidden_size\": 1024,\n",
       "  \"pos_att_type\": [\n",
       "    \"p2c\",\n",
       "    \"c2p\"\n",
       "  ],\n",
       "  \"position_biased_input\": false,\n",
       "  \"position_buckets\": 256,\n",
       "  \"relative_attention\": true,\n",
       "  \"share_att_key\": true,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 0,\n",
       "  \"vocab_size\": 128015\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a894a20-7850-4fb4-b1ed-54a35a66f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "m3 = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47f591b2-52b8-4af9-94aa-6758f5115567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\alcka\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "t_ = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "378d3dea-183e-41dd-a547-068aa29fa8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a3e7fa20-ab17-4283-bc98-afa98ff9d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6d2bfd-9e54-4cdc-9f49-635d5c4d09cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\alcka\\anaconda3\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "m4 = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "t__ = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "926477ad-40d5-489f-9d97-6ca7914f47b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2Config {\n",
       "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-07,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"max_relative_positions\": -1,\n",
       "  \"model_type\": \"deberta-v2\",\n",
       "  \"norm_rel_ebd\": \"layer_norm\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 24,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pooler_dropout\": 0,\n",
       "  \"pooler_hidden_act\": \"gelu\",\n",
       "  \"pooler_hidden_size\": 1024,\n",
       "  \"pos_att_type\": [\n",
       "    \"p2c\",\n",
       "    \"c2p\"\n",
       "  ],\n",
       "  \"position_biased_input\": false,\n",
       "  \"position_buckets\": 256,\n",
       "  \"relative_attention\": true,\n",
       "  \"share_att_key\": true,\n",
       "  \"transformers_version\": \"4.26.1\",\n",
       "  \"type_vocab_size\": 0,\n",
       "  \"vocab_size\": 128100\n",
       "}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9da7f1d1-7e61-42ac-8a71-989db19da297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-large', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "93f9aeb0-a172-463a-9926-ed2f2972c1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128001"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8375779c-c9ce-47bf-a45d-9300479ad9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t__.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66578e28-1ca7-4c1a-bb89-97eef2b4aaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[BR]',\n",
       " '[GENERIC_SCHOOL]',\n",
       " '[GENERIC_SCHOOL]',\n",
       " '[SCHOOL_NAME]',\n",
       " '[STUDENT_NAME]',\n",
       " '[GENERIC_NAME]',\n",
       " '[GENERIC_NAME]',\n",
       " '[GENERIC_CITY]',\n",
       " '[LOCATION_NAME]',\n",
       " '[HOTEL_NAME]',\n",
       " '[LANGUAGE_NAME]',\n",
       " '[PROPER_NAME]',\n",
       " '[OTHER_NAME]',\n",
       " '[PROPER_NAME]',\n",
       " '[RESTAURANT_NAME]',\n",
       " '[STORE_NAME]',\n",
       " '[TEACHER_NAME]']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "630243be-7bd3-4599-bb47-2eb2cebe4f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fae657bff342f2b2e3870e8dcee31d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alcka\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:129: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alcka\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241286d3681f42348ec638a2eac6c38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alcka\\anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tmp = T5Tokenizer.from_pretrained(\"t5-small\", extra_ids=0, additional_special_tokens=[\"new_token_1\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "550062d7-83b7-4290-bdc4-2b4369ff1594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5Tokenizer(name_or_path='t5-small', vocab_size=32000, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['new_token_1']})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "effc9340-8025-4728-bbce-98f0f3ab4b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_token_1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2f03854-2084-4416-b184-adc0516af874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_token_1': 32000}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e12d71ac-9d20-4335-8fcd-880c73d72887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_token_1']\n",
      "{'new_token_1': 32000}\n",
      "['▁this', '▁is', '▁', 'a', '▁text', '▁with', 'new_token_1', '▁', ',', '▁new', '_', 'to', 'ken', '_', '2', '▁and', '▁new', '_', 'to', 'ken', '_', '3', '</s>']\n",
      "***\n",
      "['new_token_2']\n",
      "{'new_token_1': 32000, 'new_token_2': 32001}\n",
      "['▁this', '▁is', '▁', 'a', '▁text', '▁with', 'new_token_1', '▁', ',', 'new_token_2', '▁and', '▁new', '_', 'to', 'ken', '_', '3', '</s>']\n",
      "***\n",
      "['new_token_3']\n",
      "{'new_token_1': 32000, 'new_token_2': 32001, 'new_token_3': 32002}\n",
      "['▁this', '▁is', '▁', 'a', '▁text', '▁with', 'new_token_1', '▁', ',', 'new_token_2', '▁and', 'new_token_3', '</s>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tmp\n",
    "text = \"this is a text with new_token_1, new_token_2 and new_token_3 \"\n",
    "\n",
    "print(tokenizer.additional_special_tokens)\n",
    "print(tokenizer.added_tokens_encoder)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(text)))\n",
    "print(\"***\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"new_token_2\"]})\n",
    "print(tokenizer.additional_special_tokens)\n",
    "print(tokenizer.added_tokens_encoder)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(text)))\n",
    "print(\"***\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": [\"new_token_3\"]})\n",
    "print(tokenizer.additional_special_tokens)\n",
    "print(tokenizer.added_tokens_encoder)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eff5780c-326a-4ac2-8302-ac5d0e28ffc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32003"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0473b437-a42d-4035-95e8-f0b2ff2711e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc7cb60c-1630-4315-a8f9-12f2eb68bd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new_token_3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.additional_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f5a18ba-d735-47cf-bc06-8e3f773106ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new_token_1': 32000, 'new_token_2': 32001, 'new_token_3': 32002}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.added_tokens_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a97112e-09fd-4183-9cb7-47b2c5f5c265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

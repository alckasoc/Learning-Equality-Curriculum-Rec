{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc61753-4030-47f3-9d60-3381e5287e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b76a27-63a0-45bb-b199-0862fbdc082c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbf5455-7a44-48ee-ad21-8244b8627c1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(148659456, 148659456, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_param_counts\n",
    "get_param_counts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac5334-d21c-4c26-a40a-14b92f25e44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/723 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(278043648, 278043648, 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
    "get_param_counts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a657fb6-6185-4c03-ba98-13e9b61f2c06",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff34041c48e4b9ca1b5874b2c03c0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd1ec3c74404431b50081a60688a11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(22713216, 22713216, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "get_param_counts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f495ba6-8bab-4f6e-856c-03a831f4d4b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8078df189544a48fb4dd74fe41e30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/773 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380588f50ea9410a9e6af22ddd1db02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at markussagen/xlm-roberta-longformer-base-4096 were not used when initializing XLMRobertaModel: ['lm_head.bias', 'roberta.encoder.layer.8.attention.self.value_global.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.1.attention.self.query_global.bias', 'roberta.encoder.layer.11.attention.self.key_global.weight', 'roberta.encoder.layer.10.attention.self.value_global.bias', 'roberta.encoder.layer.5.attention.self.key_global.weight', 'roberta.encoder.layer.5.attention.self.key_global.bias', 'roberta.encoder.layer.9.attention.self.key_global.weight', 'roberta.encoder.layer.6.attention.self.value_global.weight', 'roberta.encoder.layer.7.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.self.value_global.weight', 'roberta.encoder.layer.2.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.self.query_global.weight', 'roberta.encoder.layer.1.attention.self.value_global.bias', 'roberta.encoder.layer.0.attention.self.key_global.weight', 'roberta.encoder.layer.2.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.key_global.bias', 'roberta.encoder.layer.3.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.key_global.weight', 'roberta.encoder.layer.2.attention.self.query_global.weight', 'roberta.encoder.layer.11.attention.self.query_global.weight', 'lm_head.decoder.bias', 'roberta.encoder.layer.11.attention.self.key_global.bias', 'roberta.encoder.layer.1.attention.self.value_global.weight', 'roberta.encoder.layer.8.attention.self.key_global.bias', 'roberta.encoder.layer.5.attention.self.value_global.bias', 'roberta.encoder.layer.9.attention.self.value_global.weight', 'roberta.encoder.layer.1.attention.self.key_global.bias', 'roberta.encoder.layer.7.attention.self.key_global.bias', 'roberta.encoder.layer.5.attention.self.value_global.weight', 'roberta.encoder.layer.0.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.self.query_global.weight', 'roberta.encoder.layer.4.attention.self.value_global.weight', 'roberta.encoder.layer.4.attention.self.key_global.bias', 'roberta.encoder.layer.9.attention.self.query_global.bias', 'roberta.encoder.layer.7.attention.self.query_global.bias', 'roberta.encoder.layer.3.attention.self.key_global.weight', 'roberta.encoder.layer.10.attention.self.query_global.bias', 'roberta.encoder.layer.5.attention.self.query_global.bias', 'roberta.encoder.layer.9.attention.self.query_global.weight', 'roberta.encoder.layer.5.attention.self.query_global.weight', 'roberta.encoder.layer.6.attention.self.value_global.bias', 'roberta.encoder.layer.4.attention.self.key_global.weight', 'roberta.encoder.layer.10.attention.self.query_global.weight', 'roberta.encoder.layer.4.attention.self.query_global.bias', 'roberta.encoder.layer.2.attention.self.key_global.weight', 'roberta.encoder.layer.3.attention.self.value_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.bias', 'roberta.encoder.layer.1.attention.self.query_global.weight', 'roberta.encoder.layer.0.attention.self.query_global.bias', 'lm_head.dense.bias', 'roberta.encoder.layer.3.attention.self.query_global.bias', 'roberta.encoder.layer.0.attention.self.key_global.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.3.attention.self.query_global.weight', 'roberta.encoder.layer.7.attention.self.query_global.weight', 'roberta.encoder.layer.9.attention.self.key_global.bias', 'roberta.encoder.layer.2.attention.self.key_global.bias', 'roberta.encoder.layer.10.attention.self.key_global.weight', 'roberta.encoder.layer.7.attention.self.value_global.weight', 'roberta.encoder.layer.6.attention.self.query_global.bias', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.11.attention.self.query_global.bias', 'roberta.encoder.layer.6.attention.self.key_global.weight', 'roberta.encoder.layer.8.attention.self.key_global.weight', 'roberta.encoder.layer.3.attention.self.key_global.bias', 'roberta.encoder.layer.2.attention.self.query_global.bias', 'roberta.encoder.layer.4.attention.self.value_global.bias', 'roberta.encoder.layer.8.attention.self.query_global.bias', 'roberta.encoder.layer.9.attention.self.value_global.bias', 'roberta.encoder.layer.10.attention.self.value_global.weight', 'roberta.encoder.layer.0.attention.self.value_global.weight', 'roberta.encoder.layer.11.attention.self.value_global.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at markussagen/xlm-roberta-longformer-base-4096 and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(280796160, 280796160, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = AutoModel.from_pretrained(\"markussagen/xlm-roberta-longformer-base-4096\")\n",
    "get_param_counts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d49c599-6f97-461e-bffe-707dfc7726c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models.model import LongformerForTokenClassificationwithbiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcad4b8e-0b92-4838-aaf2-3fffaefb6851",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500 were not used when initializing LongformerForTokenClassificationwithbiLSTM: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForTokenClassificationwithbiLSTM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForTokenClassificationwithbiLSTM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "curr_path = \"../../input/longformerwithbilstmhead/aug-longformer-large-4096-f0/checkpoint-5500\"\n",
    "model = LongformerForTokenClassificationwithbiLSTM.from_pretrained(curr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd83208-76db-4abd-a354-76ace4cf6333",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1697789-b370-445f-901e-804297c17cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b8924-6ae2-4825-aa2a-354070983446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d32ab-23e3-406a-9997-dbf898b029f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2073735-f150-4f56-9cb9-08c40a55b724",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "model = nn.Sequential(\n",
    "    *list(model.children())[:-2], \n",
    "    nn.Linear(in_features=1024, out_features=1, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8d3a15a-d301-46ab-ac61-b4db5971a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(433552385, 433552385, 0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_param_counts\n",
    "\n",
    "get_param_counts(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "43f8ac19-c565-4263-b9d5-307216522ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = ({'input_ids': torch.Tensor([[    0, 48962, 14602,   959, 32316,  1065,   294, 21290,   268, 16734,\n",
    "         17991, 14602,   959, 32316,     2,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
    "             1,     1]]).to(torch.int32), 'attention_mask': torch.Tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0]]).to(torch.int32)},\n",
    " torch.Tensor([0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d7b769a-6843-47ec-ba03-895f1aa166a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model(**x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "967e9580-3f88-42b2-9561-d8e288ad43f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongformerTokenClassifierOutput(loss=None, logits=tensor([[[ 3.7575, -2.7763,  0.1980,  ..., -0.9218, -3.5692, -0.9446],\n",
       "         [ 4.9944, -2.8579, -0.0577,  ..., -1.4394, -3.9286, -1.4000],\n",
       "         [ 5.3115, -3.1997, -0.0754,  ..., -1.4942, -4.1202, -1.4676],\n",
       "         ...,\n",
       "         [ 2.9347, -2.2306, -0.2864,  ...,  0.2584, -2.3156, -0.6862],\n",
       "         [ 2.7385, -2.0543, -0.2111,  ...,  0.2190, -2.1398, -0.6520],\n",
       "         [ 2.3695, -1.7230, -0.1052,  ...,  0.1300, -1.7939, -0.5427]]]), hidden_states=None, attentions=None, global_attentions=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6158494f-306a-41e3-9e6a-49d8e9b1cd80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
